{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O56jp96I0MQo"
   },
   "source": [
    "# Assignment B.5 - Modern face recognition with deep learning\n",
    "\n",
    "Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic. This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing: these algorithms can recognize faces with 98% accuracy, which is pretty much as good as humans can do!\n",
    "\n",
    "As a human, your brain is wired to recognize faces automatically and instantly. Computers are not capable of doing this, so you have to teach them how to tackle each step in this process. Specifically, a face recognition system goes through four steps: find faces in the image, analyze their facial features, compare against known faces, and make a prediction of the corresponding persons. Here's described the full pipeline.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/summary.gif\" style=\"height:200px;\">\n",
    "\n",
    "### Table of contents \n",
    "\n",
    "In this assignment, you will tackle several problems related to face recognition:\n",
    "1. **Face detection**. Look at a picture and find all the faces in it. -5\n",
    "- **Pose estimation**. Understand where the face is turned and correct its pose. 5\n",
    "- **Face encoding**. Pick up unique features from a face that can be used to distinguish it from others. 5 \n",
    "- **Face recognition**. Compare the unique features of a face to those of all the people in a database. 5\n",
    "- **Personal dataset**. Build a custom face recognition dataset. 2\n",
    "\n",
    "By the end of this notebook, you will have your own face recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxS9du380MQw"
   },
   "source": [
    "### Required packages\n",
    "\n",
    "Here are the packages you will need during the assignment.\n",
    "- [Numpy](http://www.numpy.org)\n",
    "- [Keras](https://keras.io)\n",
    "- [OpenCV](https://opencv.org)\n",
    "- [Dlib](http://dlib.net).\n",
    "\n",
    "**Note**: In Anaconda Navigator, the package `dlib` can be installed from the **conda-forge** channel. In Google Colab, everything is readily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "cy3onABl0MQ0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yohan\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\yohan\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\yohan\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnCc8P7y0MQ1"
   },
   "source": [
    "## 1. Face detection\n",
    "\n",
    "The first step in your pipeline is face detection. Obviously you need to locate the faces in a photograph before you can try to tell them apart. If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action. Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But you’ll use it for a different purpose:  finding the areas of the image you want to pass on to the next step in your pipeline.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/detection.jpg\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A5esp2s0MQ2"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- You are provided with a small dataset of pictures, where each picture contains exactly one face. Extract the faces and their labels (i.e., the person's names). Store them to a new file with the function `dump` in the package `pickle`.\n",
    "\n",
    "\n",
    "-  Normalize the cropped faces (i.e., divide the pixel values by 255), and split them in train set (70%) and test set (30%) with the function `train_test_split` in the package `sklearn`. \n",
    "\n",
    "\n",
    "- Train a small convnet and check its performance on the test set. Remember: don't use the test images for training.\n",
    "\n",
    "\n",
    "- Try to improve the performance of the baseline convnet by using all the tricks you have learned in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfqaWaKR0MQ3"
   },
   "source": [
    "#### Provided functions\n",
    "\n",
    "Here you will find some useful functions to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6596,
     "status": "ok",
     "timestamp": 1612371866903,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "79wTlPUP2SDU",
    "outputId": "f7c8fa44-631a-487e-c051-a70b21b58b61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'unzip' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!wget https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n",
    "!unzip models.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0RW7kXCj0MQ4"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat')\n",
    "\n",
    "def face_locations(image, model=\"hog\"):\n",
    "    \n",
    "    if model == \"hog\":\n",
    "        detector = hog_detector\n",
    "        cst = 0\n",
    "    elif model == \"cnn\":\n",
    "        detector = cnn_detector\n",
    "        cst = 10\n",
    "            \n",
    "    matches = detector(image,1)\n",
    "    rects   = []\n",
    "    \n",
    "    for r in matches:\n",
    "        if model == \"cnn\":\n",
    "            r = r.rect\n",
    "        x = max(r.left(), 0)\n",
    "        y = max(r.top(), 0)\n",
    "        w = min(r.right(), image.shape[1]) - x + cst\n",
    "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
    "        rects.append((x,y,w,h))\n",
    "        \n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "S4HSi9Qc0MQ6"
   },
   "outputs": [],
   "source": [
    "def extract_faces(image, model=\"hog\"):\n",
    "    \n",
    "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = face_locations(gray, model)\n",
    "    faces = []\n",
    "    \n",
    "    for (x,y,w,h) in rects:\n",
    "        cropped = image[y:y+h, x:x+w, :]\n",
    "        cropped = cv2.resize(cropped, (128,128))\n",
    "        faces.append(cropped)\n",
    "            \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "k7v9sVPa0MQ7"
   },
   "outputs": [],
   "source": [
    "def show_grid(faces, figsize=(12,3)):\n",
    "\n",
    "    n = len(faces)\n",
    "    cols = 7\n",
    "    rows = int(np.ceil(n/cols))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            i = r*cols + c\n",
    "            if i == n:\n",
    "                 break\n",
    "            ax[r,c].imshow(faces[i])\n",
    "            ax[r,c].axis('off')\n",
    "            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UDMX9I2Y0MQ7"
   },
   "outputs": [],
   "source": [
    "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
    "    \n",
    "    imagePaths = []\n",
    "    \n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
    "                imagePaths.append(imagePath)\n",
    "    \n",
    "    return imagePaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMuqxVrJ0MQ9"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `extract_faces()` applies face detection to a single input image, and returns a list of 128x128 blocks containing the detected faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2646,
     "status": "ok",
     "timestamp": 1612372008331,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "ybpSOApP29TS",
    "outputId": "fd6c38c0-23f5-427e-8168-ef632ce19598"
   },
   "outputs": [],
   "source": [
    "!wget https://perso.esiee.fr/~najmanl/FaceRecognition/figures.zip\n",
    "!unzip figures.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fi1aVAyp0MQ9"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(\"figures/faces.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1484,
     "status": "ok",
     "timestamp": 1612372015155,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "AJtMFU3W0MQ-",
    "outputId": "ea0f9d9a-8aa3-4bd2-f58e-5c4280d40365"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UwDDUNJ60MQ-"
   },
   "outputs": [],
   "source": [
    "faces = extract_faces(image, \"cnn\")  # Replace 'cnn' with 'hog' for faster but less accurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1612372029514,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "EeJIoFF20MQ_",
    "outputId": "3ccb9007-65d5-420a-b992-c8d6661e1477"
   },
   "outputs": [],
   "source": [
    "show_grid(faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfp7kZK10MQ_"
   },
   "source": [
    "Moreover, the function `list_images()` locates all the jpeg/png/tiff files in a given folder (including its subfolders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6488,
     "status": "ok",
     "timestamp": 1612372080679,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "VE-KvzQv3To6",
    "outputId": "dfbae929-ca38-42c5-f99e-2a361558f069"
   },
   "outputs": [],
   "source": [
    "!wget https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1612372085626,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "Qkj4VLbT0MRA",
    "outputId": "3211df21-40c5-4915-bc0d-3228ef15939c"
   },
   "outputs": [],
   "source": [
    "imagePaths = list_images(\"data\")\n",
    "imagePaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFrGlU2q0MRB"
   },
   "source": [
    "## 2. Pose estimation\n",
    "\n",
    "You have isolated the faces in our image. But now you have to deal with the problem that faces turned different directions look totally different to a computer. To account for this, you will try to warp each picture so that the eyes and lips are always in the same place in the image. More concretely, you are going to use an algorithm called face landmark estimation. The basic idea is to locate 68 specific points (called landmarks) that exist on every face:  the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then, you’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. This will make face recognition more accurate.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/pose.png\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLBTOO7F0MRB"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- Further preprocess the face pictures by correcting their pose. You should now have a dataset of cropped, aligned, and normalized faces.\n",
    "\n",
    "\n",
    "- Re-train your convnets on the modified dataset. \n",
    "\n",
    "\n",
    "- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEeUbQSn0MRB"
   },
   "source": [
    "#### Provided functions\n",
    "\n",
    "Here you will find some useful functions to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BgIWJ13c0MRB"
   },
   "outputs": [],
   "source": [
    "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
    "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n",
    "\n",
    "def face_landmarks(face, model=\"large\"):\n",
    "    \n",
    "    if model == \"large\":\n",
    "        predictor = pose68\n",
    "    elif model == \"small\":\n",
    "        predictor = pose05\n",
    "    \n",
    "    if not isinstance(face, list):\n",
    "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
    "        return predictor(face, rect)\n",
    "    else:\n",
    "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
    "        return [predictor(f,rect) for f in face]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zYW708ym0MRB"
   },
   "outputs": [],
   "source": [
    "def shape_to_coords(shape):\n",
    "    return np.float32([[p.x, p.y] for p in shape.parts()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0n4wwxOG0MRC"
   },
   "outputs": [],
   "source": [
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
    "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0_75xONz0MRC"
   },
   "outputs": [],
   "source": [
    "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
    "    faces = []\n",
    "    for (img, marks) in zip(images, landmarks):\n",
    "        imgDim = img.shape[0]\n",
    "        coords = shape_to_coords(marks)\n",
    "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
    "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
    "        faces.append(warped)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChI8odM8YMu6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5S7-ag80MRC"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `face_landmarks()` computes the landmarks for a list of cropped faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kfLRvhHk0MRE"
   },
   "outputs": [],
   "source": [
    "landmarks = face_landmarks(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5jItizFr0MRE"
   },
   "outputs": [],
   "source": [
    "new_faces = []\n",
    "for (face,shape) in zip(faces, landmarks):\n",
    "    canvas = face.copy()\n",
    "    coords = shape_to_coords(shape)\n",
    "    for p in coords:\n",
    "        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n",
    "    new_faces.append(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1888,
     "status": "ok",
     "timestamp": 1612372111526,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "1a4Sq-uE0MRG",
    "outputId": "3b7f1496-dcaa-4d93-890c-2eaee25ba082"
   },
   "outputs": [],
   "source": [
    "show_grid(new_faces, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vk43JDV10MRG"
   },
   "outputs": [],
   "source": [
    "aligned = align_faces(faces, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1437,
     "status": "ok",
     "timestamp": 1612372118121,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "YSFdgxSO0MRG",
    "outputId": "e1dd8d8c-3430-43dd-be06-49534363949c"
   },
   "outputs": [],
   "source": [
    "show_grid(aligned, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1612372120733,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "bfTSgga90MRH",
    "outputId": "79aa80d2-b1b4-4fc6-be9b-26a3c7c0af57"
   },
   "outputs": [],
   "source": [
    "plt.imshow( np.stack(aligned, axis=3).astype(np.float32).mean(axis=3)/255 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCQgD33c0MRH"
   },
   "source": [
    "## 3. Face encoding\n",
    "\n",
    "The simplest approach to face recognition is to directly classify an unknown face with a convnet trained on your database of tagged people. Seems like a pretty good idea, right? There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly train such a big convnet. That would take way too long. What you need is a way to extract a few basic measurements from each face, which you can then use to quickly compare the unknown face with your database. For example, you might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. However, it turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n",
    "\n",
    "The solution is to train a convnet. But instead of training the network to classify pictures, it is trained to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: the picture of a known person, another picture of the same known person, and a picture of a totally different person. Then, the algorithm looks at the measurements currently generated for each of those three images. It tweaks the neural network slightly to make sure that the measurements generated for the same person are slightly closer, and the measurements for different persons are slightly further apart. After repeating this step millions of times for millions of images of thousands of different people, the convnet learns to generate 128 measurements for each person. \n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/triplet.png\" style=\"height:400px;\">\n",
    "\n",
    "This process of training a convnet to output face encodings requires a lot of data and computer power. Even with an expensive GPU, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Fortunately, the people at [OpenFace](https://cmusatyalab.github.io/openface/) already did this and they published several trained networks which you can directly use. So all you need to do is run your face images through their pre-trained network to get the 128 measurements for each face.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/encoding.png\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo7Hduwu0MRH"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- Preprocess the cropped faces by encoding them. You should now have a dataset of cropped and encoded faces.\n",
    "\n",
    "\n",
    "- Train a neural network on the modified dataset. Since the encoded faces are just 128-length vectors, **you don't need a convnet**. Use a regular neural network with a series of fully-connected layers.\n",
    "\n",
    "\n",
    "- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWu06RRG0MRH"
   },
   "source": [
    "#### Provided functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "deJ2n5b00MRI"
   },
   "outputs": [],
   "source": [
    "cnn_encoder = dlib.face_recognition_model_v1('models/dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "def face_encoder(faces):\n",
    "    \n",
    "    landmarks = face_landmarks(faces)\n",
    "    \n",
    "    if not isinstance(faces, list):\n",
    "        return np.array(cnn_encoder.compute_face_descriptor(faces,landmarks))\n",
    "    else:\n",
    "        return np.array([cnn_encoder.compute_face_descriptor(f,l) for f,l in zip(faces,landmarks)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ujJWBu0MRJ"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `face_encoder()` computes the encodings for a list of cropped faces. Alignment and normalization are handled internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "eIm0yjpR0MRJ"
   },
   "outputs": [],
   "source": [
    "encoded_faces = face_encoder(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ou94KdUl0MRJ"
   },
   "outputs": [],
   "source": [
    "plt.plot(encoded_faces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7x18Hp30MRK"
   },
   "source": [
    "## 4. Face recognition\n",
    "\n",
    "This last step is actually the easiest one in the whole process. All you have to do is find the person in your database of known people who has the closest measurements to some test image. You can do that by using any machine learning classification algorithm, such as neaural network (as you did in the previous section), logistic regression, SVM, nearest neighbours, etc. All you need to do is training a classifier that can take in the measurements from a new test image, and tells which known person is the closest match. Running this classifier must only take milliseconds, so that you can apply it to video sequences.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/test.gif\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3IRpEzh0MRK"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "-  Train several classifiers (logistic regression, SVM, kNN, neural network) on the dataset of encoded faces (you can use the package `scikit-learn`).\n",
    "\n",
    "- Evaluate their performance on the test set, in terms of accuracy and speed.\n",
    "\n",
    "- Finally, run your best classifier on the test images and video available in the `test` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYHj2ZUZ0MRK"
   },
   "source": [
    "#### Provided functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "O68ckoUO0MRL"
   },
   "outputs": [],
   "source": [
    "def process_frame(image, mode=\"fast\"):\n",
    "    \n",
    "    # face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if mode == \"fast\":\n",
    "        matches = hog_detector(gray,1)\n",
    "    else:\n",
    "        matches = cnn_detector(gray,1)\n",
    "        matches = [m.rect for m in matches]\n",
    "        \n",
    "    for rect in matches:\n",
    "        \n",
    "        # face landmarks\n",
    "        landmarks = pose68(gray, rect)\n",
    "        \n",
    "        # face encoding\n",
    "        encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
    "        \n",
    "        # face classification\n",
    "        label = \"label\"\n",
    "        \n",
    "        # draw box\n",
    "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
    "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMPHcAJI4diD"
   },
   "source": [
    "**Note:** cv2.VideoCapture does not work in Google Colab. You can use https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi to capture video 'on the fly' with Google Colab. The following function has to be modified accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BeBprnGT0MRL"
   },
   "outputs": [],
   "source": [
    "def process_movie(video_name, mode=\"fast\"):\n",
    "    \n",
    "    video  = cv2.VideoCapture(video_name)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            # Grab a single frame of video\n",
    "            ret, frame = video.read()\n",
    "            \n",
    "            # Resize frame of video for faster processing\n",
    "            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "\n",
    "            # Quit when the input video file ends or key \"Q\" is pressed\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if not ret or key == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "            # Process frame\n",
    "            image = process_frame(frame, mode)\n",
    "\n",
    "            # Display the resulting image\n",
    "            cv2.imshow('Video', image)\n",
    "    \n",
    "    finally:\n",
    "        video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Video released\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvo3Ojs40MRL"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `process_frame()` detects and encodes all the faces in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2186,
     "status": "ok",
     "timestamp": 1612372222658,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "0jZG-Ebd307i",
    "outputId": "a50471e6-db03-4675-a95e-a30bbcc4a041"
   },
   "outputs": [],
   "source": [
    "!wget https://perso.esiee.fr/~najmanl/FaceRecognition/test.zip\n",
    "!unzip test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "daq5vowP0MRL"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(\"test/example_03.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pYUuFGq-0MRL"
   },
   "outputs": [],
   "source": [
    "processed = process_frame(image.copy())\n",
    "processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "KYiQ4X530MRM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnFPofAs0MRM"
   },
   "source": [
    "The provided function `process_frame()` detects and encodes the faces in the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1502,
     "status": "error",
     "timestamp": 1612372276913,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "fREoFcuL0MRN",
    "outputId": "acf7f997-6255-442c-95b3-f19e4796fa41"
   },
   "outputs": [],
   "source": [
    "process_movie(\"test/lunch_scene.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUt43M9a0MRN"
   },
   "source": [
    "The special input `0` can be used to access the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HylqPbF10MRN"
   },
   "outputs": [],
   "source": [
    "process_movie(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aothdVBl0MRO"
   },
   "source": [
    "## 5. Build a custom dataset\n",
    "\n",
    "So far, you have used a pre-curated dataset, where somebody did the hard work of gathering and labeling the images for you. Now, you will tackle the problem of recognizing faces of yourselves, friends, family members, colleagues, etc. To accomplish this, you need to gather examples of faces you want to recognize. You can enroll facial pictures via a webcam attached to your computer.\n",
    "\n",
    "### Assignment\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- Use your webcam to enroll face pictures of yourself, your friends, etc. To do so, you need to open the webcam, detect faces in the video stream, and save the captured face images to disk. \n",
    "\n",
    "\n",
    "- Build a dataset of reasonable size: a group of 10-15 people with 50-100 face pictures each, taken in different conditions of light, angle, emotion, etc.\n",
    "\n",
    "\n",
    "- Apply the previously developed pipeline to build your own personalized face recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2vUz-z20MRO"
   },
   "source": [
    "---\n",
    "## Credits\n",
    "\n",
    "This assignment is based on Adam Geitgey's [post](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
